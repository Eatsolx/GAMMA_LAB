{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AfnlcwOWS5l"
      },
      "source": [
        "In this work, you are required to build a GNN training pipline. Then you can truly use the Graph Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex3YFUE3Whks"
      },
      "source": [
        "First, we need to download the dataset and load data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jLg7RaF6WRTt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xrz/miniforge3/envs/PyG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "dataset = Planetoid(\"./\", \"Cora\", transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "\n",
        "x = data.x\n",
        "edge_index = data.edge_index\n",
        "edge_weight = data.edge_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MA85AI9b8yX"
      },
      "source": [
        "Then, you need to implement a GNN model. You may copy the GCNConv from your work two weeks ago, and build the model with the convolution layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B4La5uI1cQQA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import GCNConv\n",
        "class PyG_GCNConv(MessagePassing):\n",
        "  def __init__(self, in_channel, out_channel):\n",
        "    super().__init__(aggr=\"sum\")\n",
        "    self.lin = nn.Linear(in_channel, out_channel, bias=True)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.lin.weight, gain=1.414)\n",
        "    if self.lin.bias is not None:\n",
        "      nn.init.zeros_(self.lin.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, edge_weight=None):\n",
        "    x_T = x @ self.W\n",
        "    out = self.propagate(edge_index=edge_index, edge_weight=edge_weight, x=x_T)\n",
        "    out += self.lin.bias\n",
        "    return out\n",
        "\n",
        "  def message(self, x_j, edge_weight):\n",
        "    return edge_weight.unsqueeze(-1) * x_j\n",
        "\n",
        "class PyG_GCN(nn.Module):\n",
        "  def __init__(self, in_channel, hidden_channel, out_channel, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.layers.append(GCNConv(in_channel, hidden_channel))\n",
        "    for _ in range(num_layers-2):\n",
        "      self.layers.append(GCNConv(hidden_channel, hidden_channel))\n",
        "    self.layers.append(GCNConv(hidden_channel, out_channel))\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self, x, edge_index, edge_weight=None):\n",
        "    for layer in self.layers[:-1]:\n",
        "      x = layer(x, edge_index, edge_weight=edge_weight)\n",
        "      x = self.relu(x)\n",
        "    out = self.layers[-1](x, edge_index, edge_weight=edge_weight)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz1whBYkcdtx"
      },
      "source": [
        "Building the training and evaluation part, this is similar to the work in week4. Our downstream task is just node classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMlTRvwGcmFw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.9460, Train: 0.1857, Val: 0.1500, Test: 0.1750\n",
            "Epoch: 002, Loss: 1.9451, Train: 0.2643, Val: 0.1780, Test: 0.2270\n",
            "Epoch: 003, Loss: 1.9444, Train: 0.3286, Val: 0.2140, Test: 0.2700\n",
            "Epoch: 004, Loss: 1.9436, Train: 0.3786, Val: 0.2460, Test: 0.3020\n",
            "Epoch: 005, Loss: 1.9429, Train: 0.5071, Val: 0.2920, Test: 0.3400\n",
            "Epoch: 006, Loss: 1.9421, Train: 0.5643, Val: 0.3280, Test: 0.3770\n",
            "Epoch: 007, Loss: 1.9413, Train: 0.6286, Val: 0.3760, Test: 0.4180\n",
            "Epoch: 008, Loss: 1.9405, Train: 0.6643, Val: 0.4160, Test: 0.4640\n",
            "Epoch: 009, Loss: 1.9396, Train: 0.6929, Val: 0.4440, Test: 0.4960\n",
            "Epoch: 010, Loss: 1.9387, Train: 0.7214, Val: 0.4780, Test: 0.5350\n",
            "Epoch: 011, Loss: 1.9377, Train: 0.7643, Val: 0.5220, Test: 0.5710\n",
            "Epoch: 012, Loss: 1.9368, Train: 0.8071, Val: 0.5580, Test: 0.5860\n",
            "Epoch: 013, Loss: 1.9358, Train: 0.8500, Val: 0.5720, Test: 0.6060\n",
            "Epoch: 014, Loss: 1.9348, Train: 0.8714, Val: 0.5760, Test: 0.6240\n",
            "Epoch: 015, Loss: 1.9337, Train: 0.8786, Val: 0.5740, Test: 0.6240\n",
            "Epoch: 016, Loss: 1.9327, Train: 0.8857, Val: 0.5940, Test: 0.6470\n",
            "Epoch: 017, Loss: 1.9316, Train: 0.8786, Val: 0.6060, Test: 0.6480\n",
            "Epoch: 018, Loss: 1.9305, Train: 0.8929, Val: 0.6200, Test: 0.6430\n",
            "Epoch: 019, Loss: 1.9294, Train: 0.9071, Val: 0.6280, Test: 0.6430\n",
            "Epoch: 020, Loss: 1.9283, Train: 0.9143, Val: 0.6340, Test: 0.6400\n",
            "Epoch: 021, Loss: 1.9271, Train: 0.9143, Val: 0.6240, Test: 0.6400\n",
            "Epoch: 022, Loss: 1.9260, Train: 0.9214, Val: 0.6300, Test: 0.6400\n",
            "Epoch: 023, Loss: 1.9248, Train: 0.9143, Val: 0.6360, Test: 0.6580\n",
            "Epoch: 024, Loss: 1.9236, Train: 0.9143, Val: 0.6320, Test: 0.6580\n",
            "Epoch: 025, Loss: 1.9224, Train: 0.9143, Val: 0.6420, Test: 0.6640\n",
            "Epoch: 026, Loss: 1.9211, Train: 0.9143, Val: 0.6460, Test: 0.6650\n",
            "Epoch: 027, Loss: 1.9199, Train: 0.9143, Val: 0.6480, Test: 0.6690\n",
            "Epoch: 028, Loss: 1.9186, Train: 0.9143, Val: 0.6540, Test: 0.6690\n",
            "Epoch: 029, Loss: 1.9173, Train: 0.9214, Val: 0.6540, Test: 0.6690\n",
            "Epoch: 030, Loss: 1.9160, Train: 0.9214, Val: 0.6580, Test: 0.6740\n",
            "Epoch: 031, Loss: 1.9146, Train: 0.9143, Val: 0.6580, Test: 0.6740\n",
            "Epoch: 032, Loss: 1.9133, Train: 0.9214, Val: 0.6600, Test: 0.6800\n",
            "Epoch: 033, Loss: 1.9119, Train: 0.9429, Val: 0.6680, Test: 0.6820\n",
            "Epoch: 034, Loss: 1.9106, Train: 0.9429, Val: 0.6820, Test: 0.6870\n",
            "Epoch: 035, Loss: 1.9092, Train: 0.9429, Val: 0.6900, Test: 0.6940\n",
            "Epoch: 036, Loss: 1.9077, Train: 0.9500, Val: 0.7020, Test: 0.6980\n",
            "Epoch: 037, Loss: 1.9063, Train: 0.9500, Val: 0.7020, Test: 0.6980\n",
            "Epoch: 038, Loss: 1.9048, Train: 0.9500, Val: 0.7060, Test: 0.7090\n",
            "Epoch: 039, Loss: 1.9033, Train: 0.9500, Val: 0.7060, Test: 0.7090\n",
            "Epoch: 040, Loss: 1.9019, Train: 0.9500, Val: 0.7020, Test: 0.7090\n",
            "Epoch: 041, Loss: 1.9004, Train: 0.9500, Val: 0.7020, Test: 0.7090\n",
            "Epoch: 042, Loss: 1.8988, Train: 0.9500, Val: 0.7060, Test: 0.7090\n",
            "Epoch: 043, Loss: 1.8973, Train: 0.9500, Val: 0.7040, Test: 0.7090\n",
            "Epoch: 044, Loss: 1.8957, Train: 0.9500, Val: 0.7040, Test: 0.7090\n",
            "Epoch: 045, Loss: 1.8942, Train: 0.9500, Val: 0.7040, Test: 0.7090\n",
            "Epoch: 046, Loss: 1.8926, Train: 0.9500, Val: 0.7020, Test: 0.7090\n",
            "Epoch: 047, Loss: 1.8910, Train: 0.9500, Val: 0.7040, Test: 0.7090\n",
            "Epoch: 048, Loss: 1.8894, Train: 0.9500, Val: 0.7080, Test: 0.7240\n",
            "Epoch: 049, Loss: 1.8878, Train: 0.9500, Val: 0.7080, Test: 0.7240\n",
            "Epoch: 050, Loss: 1.8862, Train: 0.9500, Val: 0.7040, Test: 0.7240\n",
            "Epoch: 051, Loss: 1.8846, Train: 0.9500, Val: 0.7060, Test: 0.7240\n",
            "Epoch: 052, Loss: 1.8829, Train: 0.9500, Val: 0.7100, Test: 0.7350\n",
            "Epoch: 053, Loss: 1.8813, Train: 0.9500, Val: 0.7100, Test: 0.7350\n",
            "Epoch: 054, Loss: 1.8796, Train: 0.9500, Val: 0.7100, Test: 0.7350\n",
            "Epoch: 055, Loss: 1.8779, Train: 0.9500, Val: 0.7080, Test: 0.7350\n",
            "Epoch: 056, Loss: 1.8762, Train: 0.9500, Val: 0.7140, Test: 0.7330\n",
            "Epoch: 057, Loss: 1.8745, Train: 0.9500, Val: 0.7140, Test: 0.7330\n",
            "Epoch: 058, Loss: 1.8728, Train: 0.9500, Val: 0.7120, Test: 0.7330\n",
            "Epoch: 059, Loss: 1.8711, Train: 0.9500, Val: 0.7120, Test: 0.7330\n",
            "Epoch: 060, Loss: 1.8693, Train: 0.9500, Val: 0.7100, Test: 0.7330\n",
            "Epoch: 061, Loss: 1.8676, Train: 0.9500, Val: 0.7140, Test: 0.7330\n",
            "Epoch: 062, Loss: 1.8658, Train: 0.9500, Val: 0.7140, Test: 0.7330\n",
            "Epoch: 063, Loss: 1.8640, Train: 0.9500, Val: 0.7120, Test: 0.7330\n",
            "Epoch: 064, Loss: 1.8622, Train: 0.9500, Val: 0.7120, Test: 0.7330\n",
            "Epoch: 065, Loss: 1.8604, Train: 0.9500, Val: 0.7100, Test: 0.7330\n",
            "Epoch: 066, Loss: 1.8586, Train: 0.9500, Val: 0.7100, Test: 0.7330\n",
            "Epoch: 067, Loss: 1.8568, Train: 0.9500, Val: 0.7140, Test: 0.7330\n",
            "Epoch: 068, Loss: 1.8550, Train: 0.9500, Val: 0.7180, Test: 0.7450\n",
            "Epoch: 069, Loss: 1.8531, Train: 0.9500, Val: 0.7200, Test: 0.7460\n",
            "Epoch: 070, Loss: 1.8513, Train: 0.9429, Val: 0.7200, Test: 0.7460\n",
            "Epoch: 071, Loss: 1.8494, Train: 0.9429, Val: 0.7200, Test: 0.7460\n",
            "Epoch: 072, Loss: 1.8475, Train: 0.9429, Val: 0.7220, Test: 0.7470\n",
            "Epoch: 073, Loss: 1.8456, Train: 0.9429, Val: 0.7260, Test: 0.7490\n",
            "Epoch: 074, Loss: 1.8437, Train: 0.9429, Val: 0.7260, Test: 0.7490\n",
            "Epoch: 075, Loss: 1.8418, Train: 0.9429, Val: 0.7260, Test: 0.7490\n",
            "Epoch: 076, Loss: 1.8398, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 077, Loss: 1.8379, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 078, Loss: 1.8359, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 079, Loss: 1.8340, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 080, Loss: 1.8320, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 081, Loss: 1.8300, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 082, Loss: 1.8280, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 083, Loss: 1.8259, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 084, Loss: 1.8239, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 085, Loss: 1.8219, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 086, Loss: 1.8198, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 087, Loss: 1.8177, Train: 0.9429, Val: 0.7300, Test: 0.7500\n",
            "Epoch: 088, Loss: 1.8157, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 089, Loss: 1.8136, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 090, Loss: 1.8114, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 091, Loss: 1.8093, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 092, Loss: 1.8072, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 093, Loss: 1.8051, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 094, Loss: 1.8029, Train: 0.9429, Val: 0.7280, Test: 0.7500\n",
            "Epoch: 095, Loss: 1.8007, Train: 0.9429, Val: 0.7320, Test: 0.7480\n",
            "Epoch: 096, Loss: 1.7985, Train: 0.9429, Val: 0.7320, Test: 0.7480\n",
            "Epoch: 097, Loss: 1.7964, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 098, Loss: 1.7941, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 099, Loss: 1.7919, Train: 0.9429, Val: 0.7320, Test: 0.7480\n",
            "Epoch: 100, Loss: 1.7897, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 101, Loss: 1.7874, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 102, Loss: 1.7852, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 103, Loss: 1.7829, Train: 0.9429, Val: 0.7260, Test: 0.7480\n",
            "Epoch: 104, Loss: 1.7806, Train: 0.9429, Val: 0.7260, Test: 0.7480\n",
            "Epoch: 105, Loss: 1.7783, Train: 0.9429, Val: 0.7260, Test: 0.7480\n",
            "Epoch: 106, Loss: 1.7760, Train: 0.9429, Val: 0.7260, Test: 0.7480\n",
            "Epoch: 107, Loss: 1.7736, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 108, Loss: 1.7713, Train: 0.9429, Val: 0.7300, Test: 0.7480\n",
            "Epoch: 109, Loss: 1.7689, Train: 0.9429, Val: 0.7320, Test: 0.7480\n",
            "Epoch: 110, Loss: 1.7665, Train: 0.9429, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 111, Loss: 1.7641, Train: 0.9429, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 112, Loss: 1.7616, Train: 0.9429, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 113, Loss: 1.7591, Train: 0.9429, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 114, Loss: 1.7566, Train: 0.9429, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 115, Loss: 1.7541, Train: 0.9429, Val: 0.7360, Test: 0.7480\n",
            "Epoch: 116, Loss: 1.7516, Train: 0.9429, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 117, Loss: 1.7491, Train: 0.9429, Val: 0.7360, Test: 0.7480\n",
            "Epoch: 118, Loss: 1.7465, Train: 0.9429, Val: 0.7360, Test: 0.7480\n",
            "Epoch: 119, Loss: 1.7439, Train: 0.9429, Val: 0.7360, Test: 0.7480\n",
            "Epoch: 120, Loss: 1.7414, Train: 0.9429, Val: 0.7360, Test: 0.7480\n",
            "Epoch: 121, Loss: 1.7388, Train: 0.9429, Val: 0.7360, Test: 0.7480\n",
            "Epoch: 122, Loss: 1.7362, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 123, Loss: 1.7335, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 124, Loss: 1.7309, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 125, Loss: 1.7282, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 126, Loss: 1.7256, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 127, Loss: 1.7229, Train: 0.9429, Val: 0.7360, Test: 0.7490\n",
            "Epoch: 128, Loss: 1.7202, Train: 0.9429, Val: 0.7360, Test: 0.7490\n",
            "Epoch: 129, Loss: 1.7175, Train: 0.9429, Val: 0.7360, Test: 0.7490\n",
            "Epoch: 130, Loss: 1.7148, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 131, Loss: 1.7121, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 132, Loss: 1.7094, Train: 0.9429, Val: 0.7380, Test: 0.7490\n",
            "Epoch: 133, Loss: 1.7066, Train: 0.9429, Val: 0.7420, Test: 0.7510\n",
            "Epoch: 134, Loss: 1.7039, Train: 0.9429, Val: 0.7400, Test: 0.7510\n",
            "Epoch: 135, Loss: 1.7011, Train: 0.9429, Val: 0.7400, Test: 0.7510\n",
            "Epoch: 136, Loss: 1.6983, Train: 0.9429, Val: 0.7400, Test: 0.7510\n",
            "Epoch: 137, Loss: 1.6955, Train: 0.9429, Val: 0.7400, Test: 0.7510\n",
            "Epoch: 138, Loss: 1.6927, Train: 0.9429, Val: 0.7420, Test: 0.7510\n",
            "Epoch: 139, Loss: 1.6899, Train: 0.9429, Val: 0.7420, Test: 0.7510\n",
            "Epoch: 140, Loss: 1.6871, Train: 0.9429, Val: 0.7420, Test: 0.7510\n",
            "Epoch: 141, Loss: 1.6843, Train: 0.9429, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 142, Loss: 1.6815, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 143, Loss: 1.6786, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 144, Loss: 1.6758, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 145, Loss: 1.6729, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 146, Loss: 1.6700, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 147, Loss: 1.6671, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 148, Loss: 1.6642, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 149, Loss: 1.6613, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 150, Loss: 1.6584, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 151, Loss: 1.6555, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 152, Loss: 1.6525, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 153, Loss: 1.6496, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 154, Loss: 1.6467, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 155, Loss: 1.6437, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 156, Loss: 1.6407, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 157, Loss: 1.6377, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 158, Loss: 1.6348, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 159, Loss: 1.6318, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 160, Loss: 1.6288, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 161, Loss: 1.6258, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 162, Loss: 1.6227, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 163, Loss: 1.6197, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 164, Loss: 1.6167, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 165, Loss: 1.6137, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 166, Loss: 1.6106, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 167, Loss: 1.6076, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 168, Loss: 1.6045, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 169, Loss: 1.6014, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 170, Loss: 1.5984, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 171, Loss: 1.5953, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 172, Loss: 1.5922, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 173, Loss: 1.5891, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 174, Loss: 1.5860, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 175, Loss: 1.5829, Train: 0.9500, Val: 0.7400, Test: 0.7500\n",
            "Epoch: 176, Loss: 1.5798, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 177, Loss: 1.5767, Train: 0.9500, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 178, Loss: 1.5735, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 179, Loss: 1.5704, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 180, Loss: 1.5673, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 181, Loss: 1.5641, Train: 0.9500, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 182, Loss: 1.5610, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 183, Loss: 1.5578, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 184, Loss: 1.5547, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 185, Loss: 1.5515, Train: 0.9429, Val: 0.7420, Test: 0.7500\n",
            "Epoch: 186, Loss: 1.5484, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 187, Loss: 1.5452, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 188, Loss: 1.5420, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 189, Loss: 1.5388, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 190, Loss: 1.5357, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 191, Loss: 1.5325, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 192, Loss: 1.5293, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 193, Loss: 1.5261, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 194, Loss: 1.5229, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 195, Loss: 1.5197, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 196, Loss: 1.5165, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 197, Loss: 1.5133, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 198, Loss: 1.5101, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 199, Loss: 1.5069, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 200, Loss: 1.5037, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 201, Loss: 1.5004, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 202, Loss: 1.4972, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 203, Loss: 1.4940, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 204, Loss: 1.4908, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 205, Loss: 1.4875, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 206, Loss: 1.4843, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 207, Loss: 1.4811, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 208, Loss: 1.4778, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 209, Loss: 1.4746, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 210, Loss: 1.4714, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 211, Loss: 1.4681, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 212, Loss: 1.4649, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 213, Loss: 1.4616, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 214, Loss: 1.4584, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 215, Loss: 1.4551, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 216, Loss: 1.4519, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 217, Loss: 1.4486, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 218, Loss: 1.4454, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 219, Loss: 1.4422, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 220, Loss: 1.4389, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 221, Loss: 1.4357, Train: 0.9429, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 222, Loss: 1.4324, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 223, Loss: 1.4292, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 224, Loss: 1.4259, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 225, Loss: 1.4227, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 226, Loss: 1.4194, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 227, Loss: 1.4162, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 228, Loss: 1.4129, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 229, Loss: 1.4097, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 230, Loss: 1.4064, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 231, Loss: 1.4032, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 232, Loss: 1.4000, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 233, Loss: 1.3967, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 234, Loss: 1.3935, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 235, Loss: 1.3902, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 236, Loss: 1.3870, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 237, Loss: 1.3838, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 238, Loss: 1.3805, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 239, Loss: 1.3773, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 240, Loss: 1.3741, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 241, Loss: 1.3708, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 242, Loss: 1.3676, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 243, Loss: 1.3644, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 244, Loss: 1.3612, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 245, Loss: 1.3579, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 246, Loss: 1.3547, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 247, Loss: 1.3515, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 248, Loss: 1.3483, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 249, Loss: 1.3451, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 250, Loss: 1.3419, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 251, Loss: 1.3387, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 252, Loss: 1.3355, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 253, Loss: 1.3323, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 254, Loss: 1.3291, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 255, Loss: 1.3259, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 256, Loss: 1.3227, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 257, Loss: 1.3195, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 258, Loss: 1.3163, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 259, Loss: 1.3131, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 260, Loss: 1.3100, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 261, Loss: 1.3068, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 262, Loss: 1.3036, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 263, Loss: 1.3005, Train: 0.9429, Val: 0.7420, Test: 0.7570\n",
            "Epoch: 264, Loss: 1.2973, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 265, Loss: 1.2941, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 266, Loss: 1.2910, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 267, Loss: 1.2878, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 268, Loss: 1.2847, Train: 0.9429, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 269, Loss: 1.2815, Train: 0.9500, Val: 0.7440, Test: 0.7570\n",
            "Epoch: 270, Loss: 1.2784, Train: 0.9500, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 271, Loss: 1.2753, Train: 0.9500, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 272, Loss: 1.2721, Train: 0.9500, Val: 0.7480, Test: 0.7820\n",
            "Epoch: 273, Loss: 1.2690, Train: 0.9500, Val: 0.7480, Test: 0.7820\n",
            "Epoch: 274, Loss: 1.2659, Train: 0.9500, Val: 0.7480, Test: 0.7820\n",
            "Epoch: 275, Loss: 1.2628, Train: 0.9500, Val: 0.7480, Test: 0.7820\n",
            "Epoch: 276, Loss: 1.2597, Train: 0.9500, Val: 0.7480, Test: 0.7820\n",
            "Epoch: 277, Loss: 1.2566, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 278, Loss: 1.2535, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 279, Loss: 1.2504, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 280, Loss: 1.2473, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 281, Loss: 1.2442, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 282, Loss: 1.2411, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 283, Loss: 1.2380, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 284, Loss: 1.2350, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 285, Loss: 1.2319, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 286, Loss: 1.2288, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 287, Loss: 1.2258, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 288, Loss: 1.2227, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 289, Loss: 1.2197, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 290, Loss: 1.2166, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 291, Loss: 1.2136, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 292, Loss: 1.2106, Train: 0.9500, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 293, Loss: 1.2076, Train: 0.9500, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 294, Loss: 1.2045, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 295, Loss: 1.2015, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 296, Loss: 1.1985, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 297, Loss: 1.1955, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 298, Loss: 1.1925, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 299, Loss: 1.1895, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 300, Loss: 1.1866, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 301, Loss: 1.1836, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 302, Loss: 1.1806, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 303, Loss: 1.1777, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 304, Loss: 1.1747, Train: 0.9500, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 305, Loss: 1.1718, Train: 0.9571, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 306, Loss: 1.1688, Train: 0.9571, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 307, Loss: 1.1659, Train: 0.9571, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 308, Loss: 1.1629, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 309, Loss: 1.1600, Train: 0.9571, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 310, Loss: 1.1571, Train: 0.9571, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 311, Loss: 1.1542, Train: 0.9571, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 312, Loss: 1.1513, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 313, Loss: 1.1484, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 314, Loss: 1.1455, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 315, Loss: 1.1426, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 316, Loss: 1.1397, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 317, Loss: 1.1368, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 318, Loss: 1.1340, Train: 0.9571, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 319, Loss: 1.1311, Train: 0.9571, Val: 0.7540, Test: 0.7870\n",
            "Epoch: 320, Loss: 1.1283, Train: 0.9571, Val: 0.7540, Test: 0.7870\n",
            "Epoch: 321, Loss: 1.1254, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 322, Loss: 1.1226, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 323, Loss: 1.1198, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 324, Loss: 1.1169, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 325, Loss: 1.1141, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 326, Loss: 1.1113, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 327, Loss: 1.1085, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 328, Loss: 1.1057, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 329, Loss: 1.1029, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 330, Loss: 1.1001, Train: 0.9571, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 331, Loss: 1.0974, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 332, Loss: 1.0946, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 333, Loss: 1.0918, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 334, Loss: 1.0891, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 335, Loss: 1.0863, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 336, Loss: 1.0836, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 337, Loss: 1.0809, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 338, Loss: 1.0781, Train: 0.9571, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 339, Loss: 1.0754, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 340, Loss: 1.0727, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 341, Loss: 1.0700, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 342, Loss: 1.0673, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 343, Loss: 1.0646, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 344, Loss: 1.0619, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 345, Loss: 1.0593, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 346, Loss: 1.0566, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 347, Loss: 1.0539, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 348, Loss: 1.0513, Train: 0.9643, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 349, Loss: 1.0487, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 350, Loss: 1.0460, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 351, Loss: 1.0434, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 352, Loss: 1.0408, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 353, Loss: 1.0382, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 354, Loss: 1.0356, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 355, Loss: 1.0330, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 356, Loss: 1.0304, Train: 0.9643, Val: 0.7600, Test: 0.7900\n",
            "Epoch: 357, Loss: 1.0278, Train: 0.9643, Val: 0.7620, Test: 0.7910\n",
            "Epoch: 358, Loss: 1.0252, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 359, Loss: 1.0227, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 360, Loss: 1.0201, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 361, Loss: 1.0175, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 362, Loss: 1.0150, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 363, Loss: 1.0125, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 364, Loss: 1.0099, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 365, Loss: 1.0074, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 366, Loss: 1.0049, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 367, Loss: 1.0024, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 368, Loss: 0.9999, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 369, Loss: 0.9974, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 370, Loss: 0.9949, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 371, Loss: 0.9925, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 372, Loss: 0.9900, Train: 0.9643, Val: 0.7620, Test: 0.7910\n",
            "Epoch: 373, Loss: 0.9875, Train: 0.9643, Val: 0.7620, Test: 0.7910\n",
            "Epoch: 374, Loss: 0.9851, Train: 0.9643, Val: 0.7620, Test: 0.7910\n",
            "Epoch: 375, Loss: 0.9826, Train: 0.9643, Val: 0.7620, Test: 0.7910\n",
            "Epoch: 376, Loss: 0.9802, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 377, Loss: 0.9778, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 378, Loss: 0.9753, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 379, Loss: 0.9729, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 380, Loss: 0.9705, Train: 0.9643, Val: 0.7640, Test: 0.7910\n",
            "Epoch: 381, Loss: 0.9681, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 382, Loss: 0.9657, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 383, Loss: 0.9634, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 384, Loss: 0.9610, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 385, Loss: 0.9586, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 386, Loss: 0.9563, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 387, Loss: 0.9539, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 388, Loss: 0.9516, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 389, Loss: 0.9492, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 390, Loss: 0.9469, Train: 0.9643, Val: 0.7660, Test: 0.7930\n",
            "Epoch: 391, Loss: 0.9446, Train: 0.9643, Val: 0.7680, Test: 0.7940\n",
            "Epoch: 392, Loss: 0.9423, Train: 0.9643, Val: 0.7680, Test: 0.7940\n",
            "Epoch: 393, Loss: 0.9400, Train: 0.9643, Val: 0.7680, Test: 0.7940\n",
            "Epoch: 394, Loss: 0.9377, Train: 0.9643, Val: 0.7680, Test: 0.7940\n",
            "Epoch: 395, Loss: 0.9354, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 396, Loss: 0.9331, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 397, Loss: 0.9309, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 398, Loss: 0.9286, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 399, Loss: 0.9263, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 400, Loss: 0.9241, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 401, Loss: 0.9219, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 402, Loss: 0.9196, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 403, Loss: 0.9174, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 404, Loss: 0.9152, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 405, Loss: 0.9130, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 406, Loss: 0.9108, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 407, Loss: 0.9086, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 408, Loss: 0.9064, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 409, Loss: 0.9042, Train: 0.9643, Val: 0.7700, Test: 0.7940\n",
            "Epoch: 410, Loss: 0.9020, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 411, Loss: 0.8999, Train: 0.9643, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 412, Loss: 0.8977, Train: 0.9643, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 413, Loss: 0.8956, Train: 0.9643, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 414, Loss: 0.8934, Train: 0.9643, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 415, Loss: 0.8913, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 416, Loss: 0.8892, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 417, Loss: 0.8870, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 418, Loss: 0.8849, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 419, Loss: 0.8828, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 420, Loss: 0.8807, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 421, Loss: 0.8787, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 422, Loss: 0.8766, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 423, Loss: 0.8745, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 424, Loss: 0.8724, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 425, Loss: 0.8704, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 426, Loss: 0.8683, Train: 0.9643, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 427, Loss: 0.8663, Train: 0.9643, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 428, Loss: 0.8642, Train: 0.9643, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 429, Loss: 0.8622, Train: 0.9643, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 430, Loss: 0.8602, Train: 0.9643, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 431, Loss: 0.8582, Train: 0.9714, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 432, Loss: 0.8562, Train: 0.9714, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 433, Loss: 0.8542, Train: 0.9714, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 434, Loss: 0.8522, Train: 0.9714, Val: 0.7700, Test: 0.7970\n",
            "Epoch: 435, Loss: 0.8502, Train: 0.9714, Val: 0.7720, Test: 0.7970\n",
            "Epoch: 436, Loss: 0.8482, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 437, Loss: 0.8463, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 438, Loss: 0.8443, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 439, Loss: 0.8423, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 440, Loss: 0.8404, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 441, Loss: 0.8385, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 442, Loss: 0.8365, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 443, Loss: 0.8346, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 444, Loss: 0.8327, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 445, Loss: 0.8308, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 446, Loss: 0.8289, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 447, Loss: 0.8270, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 448, Loss: 0.8251, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 449, Loss: 0.8232, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 450, Loss: 0.8213, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 451, Loss: 0.8195, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 452, Loss: 0.8176, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 453, Loss: 0.8157, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 454, Loss: 0.8139, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 455, Loss: 0.8121, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 456, Loss: 0.8102, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 457, Loss: 0.8084, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 458, Loss: 0.8066, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 459, Loss: 0.8048, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 460, Loss: 0.8030, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 461, Loss: 0.8012, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 462, Loss: 0.7994, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 463, Loss: 0.7976, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 464, Loss: 0.7958, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 465, Loss: 0.7940, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 466, Loss: 0.7923, Train: 0.9714, Val: 0.7740, Test: 0.7970\n",
            "Epoch: 467, Loss: 0.7905, Train: 0.9714, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 468, Loss: 0.7888, Train: 0.9714, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 469, Loss: 0.7870, Train: 0.9714, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 470, Loss: 0.7853, Train: 0.9714, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 471, Loss: 0.7835, Train: 0.9714, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 472, Loss: 0.7818, Train: 0.9714, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 473, Loss: 0.7801, Train: 0.9786, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 474, Loss: 0.7784, Train: 0.9786, Val: 0.7760, Test: 0.8040\n",
            "Epoch: 475, Loss: 0.7767, Train: 0.9786, Val: 0.7780, Test: 0.8070\n",
            "Epoch: 476, Loss: 0.7750, Train: 0.9786, Val: 0.7780, Test: 0.8070\n",
            "Epoch: 477, Loss: 0.7733, Train: 0.9786, Val: 0.7780, Test: 0.8070\n",
            "Epoch: 478, Loss: 0.7716, Train: 0.9786, Val: 0.7780, Test: 0.8070\n",
            "Epoch: 479, Loss: 0.7699, Train: 0.9786, Val: 0.7780, Test: 0.8070\n",
            "Epoch: 480, Loss: 0.7682, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 481, Loss: 0.7666, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 482, Loss: 0.7649, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 483, Loss: 0.7633, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 484, Loss: 0.7616, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 485, Loss: 0.7600, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 486, Loss: 0.7583, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 487, Loss: 0.7567, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 488, Loss: 0.7551, Train: 0.9786, Val: 0.7800, Test: 0.8080\n",
            "Epoch: 489, Loss: 0.7535, Train: 0.9786, Val: 0.7820, Test: 0.8090\n",
            "Epoch: 490, Loss: 0.7518, Train: 0.9786, Val: 0.7820, Test: 0.8090\n",
            "Epoch: 491, Loss: 0.7502, Train: 0.9786, Val: 0.7820, Test: 0.8090\n",
            "Epoch: 492, Loss: 0.7486, Train: 0.9786, Val: 0.7820, Test: 0.8090\n",
            "Epoch: 493, Loss: 0.7470, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 494, Loss: 0.7454, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 495, Loss: 0.7439, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 496, Loss: 0.7423, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 497, Loss: 0.7407, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 498, Loss: 0.7391, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 499, Loss: 0.7376, Train: 0.9786, Val: 0.7840, Test: 0.8090\n",
            "Epoch: 500, Loss: 0.7360, Train: 0.9786, Val: 0.7840, Test: 0.8090\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.logging import log\n",
        "# Build your training pipeline\n",
        "hidden_dim = 16\n",
        "lr = 0.001\n",
        "epochs = 100\n",
        "model = PyG_GCN(dataset.num_features, hidden_dim, dataset.num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4, lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "best_val_acc = 0.0\n",
        "test_acc = 0.0\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  out = model(x, edge_index, edge_weight)\n",
        "  loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "  model.eval()\n",
        "  pred = model(x, edge_index, edge_weight).argmax(dim=-1)\n",
        "\n",
        "  accs = []\n",
        "  for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "      accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "  return accs\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  loss = train()\n",
        "  train_acc, val_acc, tmp_test_acc = test()\n",
        "  if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      test_acc = tmp_test_acc\n",
        "  log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pav74NceOl5"
      },
      "source": [
        "Now, you can train the GCN model with PyG. Next, you may try using the DGL to implement the similiar function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNDS5VDreXgC"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "import dgl\n",
        "import dgl.nn as dglnn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl import AddSelfLoop\n",
        "from dgl.data import CoraGraphDataset\n",
        "\n",
        "transform = (\n",
        "        AddSelfLoop()\n",
        "    )\n",
        "data = CoraGraphDataset(transform=transform)\n",
        "g = data[0]\n",
        "features = g.ndata[\"feat\"]\n",
        "labels = g.ndata[\"label\"]\n",
        "masks = g.ndata[\"train_mask\"], g.ndata[\"val_mask\"], g.ndata[\"test_mask\"]\n",
        "\n",
        "\n",
        "class DGL_GCNConv(nn.Module):\n",
        "  # Your code here\n",
        "  pass\n",
        "  # End code here\n",
        "\n",
        "class DGL_GCN(nn.Module):\n",
        "  # Your code here\n",
        "  pass\n",
        "  # End code here\n",
        "\n",
        "def train(g, features, labels, masks, model):\n",
        "  # Your code here\n",
        "  pass\n",
        "  # End code here\n",
        "\n",
        "def evaluate(g, features, labels, mask, model):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits = model(g, features)\n",
        "    logits = logits[mask]\n",
        "    labels = labels[mask]\n",
        "    _, indices = torch.max(logits, dim=1)\n",
        "    correct = torch.sum(indices == labels)\n",
        "    return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "model = DGL_GCN(features.shape[1], 16)\n",
        "print(\"Training...\")\n",
        "train(g, features, labels, masks, model)\n",
        "\n",
        "# test the model\n",
        "print(\"Testing...\")\n",
        "acc = evaluate(g, features, labels, masks[2], model)\n",
        "print(\"Test accuracy {:.4f}\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG3acaBaf0Uj"
      },
      "source": [
        "If you find it hard to implement, you may refer to the official implementation of the GNN training, like [PyG](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py) and [DGL](https://github.com/dmlc/dgl/blob/master/examples/pytorch/gcn/train.py)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PyG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
